# References

Bulut, O., Guo, Q., & Gierl, M. J. (2017). A structural equation modeling approach for examining position effects in large-scale assessments. Large-Scale Assessments in Education, 5(8), 1-20. doi:10.1186/s40536-017-0042-x

Bulut, O., Lei, M., & Guo, Q. (2018). Item and testlet position effects in computer-based alternate assessments for students with disabilities. International Journal of Research & Method in Education, 41(2), 169-183. doi: 10.1080/1743727X.2016.1262341

Choe, E. M., Zhang, J., & Chang, H. H. (2018). Sequential detection of compromised items using response times in computerized adaptive testing. Psychometrika, 83(3), 650-673.

Gao, Y., Cui, Y. Bulut, O., Zhai, X., & Chen, F. (2022). Examining adults' web navigation patterns in multi-layered hypertext environments. Computers in Human Behavior, 129, 107142. doi:10.1016/j.chb.2021.107142

Gabadinho, A., Ritschard, G., Müller, N. S., & Studer, M. (2011). Analyzing and visualizing state sequences in R with TraMineR. Journal of statistical software, 40(4), 1-37.

Gabadinho, A., Ritschard, G., Studer, M., & Müller, N. S. (2009, October). Extracting and rendering representative sequences. In International Joint Conference on Knowledge Discovery, Knowledge Engineering, and Knowledge Management (pp. 94-106). Springer, Berlin, Heidelberg.

Goldhammer, F., Hahnel, C., Kroehne, U., & Zehner, F. (2021). From byproduct to design factor: on validating the interpretation of process indicators based on log data. Large-scale Assessments in Education, 9(1), 1-25.

Gorgun, G., & Bulut, O. (2021). A polytomous scoring approach to handle not-reached items in low-stakes assessments. Educational and Psychological Measurement, 81(5), 847-871. 

Jenkins, C., Corritore, C. L., & Wiedenbeck, S. (2003). Patterns of information seeking on the Web: A qualitative study of domain expertise and Web expertise. IT & society, 1(3), 64-89.

Juvina, I., & van Oostendorp, H. (2006). Individual differences and behavioral metrics involved in modeling web navigation. Universal Access in the Information Society, 4, 258–269.

Kan, A., Bulut, O., & Cormier, D. C. (2018). The impact of item stem format on the dimensional structure of mathematics assessments. Educational Assessment, 24(1), 13-32. doi: 10.1080/10627197.2018.1545569

Kroehne, U., Deribo, T., & Goldhammer, F. (2020). Rapid guessing rates across administration mode and test setting. Psychological Test and Assessment Modeling, 62(2), 147-177.

Liou, P.-Y., & Bulut, O. (2020). The effects of item format and cognitive domain on students’ science performance in TIMSS 2011. Research in Science Education, 50(1), 99-121. doi: 10.1007/s11165-017-9682-7

Mostow, J., & Beck, J. E. (2009). Why, What, and How to Log? Lessons from LISTEN. International Working Group on Educational Data Mining.

Reader, W. R., & Payne, S. J. (2007). Allocating time across multiple texts: Sampling and satisficing. Human–Computer Interaction, 22(3), 263-298.

Schnipke, D. L., & Scrams, D. J. (1997). Modeling item response times with a two-state mixture model: A new method of measuring speededness. Journal of Educational Measurement, 34, 213–232.

Shin, J., Bulut, O., & Gierl, M. J. (2020). The effect of the most-attractive-distractor location on multiple-choice item difficulty. Journal of Experimental Education, 88(4), 643-659. doi:10.1080/00220973.2019.1629577

Sinharay, S. (2020). Detection of item preknowledge using response times. Applied Psychological Measurement, 44(5), 376-392.

Sinharay, S. (2021). Latent-variable Approaches Utilizing Both Item Scores and Response Times To Detect Test Fraud. Open Education Studies, 3(1), 1-16.

Tijmstra, J., & Bolsinova, M. (2018). On the importance of the speed-ability trade-off when dealing with not reached items. Frontiers in psychology, 9, 964.

Ulitzsch, E., von Davier, M., & Pohl, S. (2020). A hierarchical latent response model for inferences about examinee engagement in terms of guessing and item‐level non‐response. British Journal of Mathematical and Statistical Psychology, 73, 83-112.

Wise, S. L., & Kingsbury, G. G. (2016). Modeling student test‐taking motivation in the context of an adaptive achievement test. Journal of Educational Measurement, 53(1), 86-105.

Wise, S. L., & Kong, X. (2005). Response time effort: A new measure of examinee motivation in computer-based tests. Applied Measurement in Education, 18, 163–183.

Wise, S. L., Ma, L. (2012). Setting response time thresholds for a CAT item pool: The normative threshold method. Paper presented at the annual meeting of the National Council on Measurement in Education, Vancouver, BC, Canada.

Wise, S. L., Ma, L., & Theaker, R. A. (2014). Identifying non-effortful student behavior on adaptive tests: Implications for test fraud detection. In Test Fraud (pp. 191-201). Routledge.

Yildirim-Erbasli, S. N., & Bulut, O. (2021). The impact of students' test-taking effort on growth estimates in low-stakes educational assessments. Educational Research and Evaluation, 26(7-8), 368-386. doi:10.1080/13803611.2021.1977152

Yildirim-Erbasli, S. N., & Bulut, O. (In press). Designing predictive models for early prediction of students’ test-taking engagement in computerized formative assessments. Journal of Applied Testing Technology. 

Zopluoglu, C. (2019). Detecting examinees with item preknowledge in large-scale testing using extreme gradient boosting (XGBoost). Educational and psychological measurement, 79(5), 931-961.
